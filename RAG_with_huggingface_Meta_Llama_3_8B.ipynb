{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBeBsTm5+xTi2ZHmhxbr/C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammadalinoor-1982/Blog/blob/master/RAG_with_huggingface_Meta_Llama_3_8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step-by-Step Code of RAG pipeline with huggingface_Meta-Llama-3-8B**\n",
        "\n"
      ],
      "metadata": {
        "id": "BHHsgetMWzWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Install Necessary Libraries**"
      ],
      "metadata": {
        "id": "-DDo4PNocaLi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeTX143gVT1r",
        "outputId": "76fde746-9b89-4d14-96c9-ebf294f40931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf, faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.2 pymupdf-1.26.7\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Import Libraries and Setup Environment**"
      ],
      "metadata": {
        "id": "DQAvwHxvcorY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import nltk\n",
        "import sqlite3\n",
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "jvaOjTroWmUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. Database Operations**"
      ],
      "metadata": {
        "id": "3iIlQ6Slcz-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_database(db_name=\"documents.db\"):\n",
        "    conn = sqlite3.connect(db_name)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('''CREATE TABLE IF NOT EXISTS chunks\n",
        "                      (id INTEGER PRIMARY KEY, content TEXT)''')\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(f\"Database '{db_name}' created with table 'chunks'.\")\n",
        "\n",
        "def insert_chunks(chunks, db_name=\"documents.db\"):\n",
        "    conn = sqlite3.connect(db_name)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.executemany(\"INSERT INTO chunks (content) VALUES (?)\", [(chunk,) for chunk in chunks])\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(f\"Inserted {len(chunks)} chunks into 'chunks' table.\")"
      ],
      "metadata": {
        "id": "Brp-P174WmSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4. Text Extraction and Chunking**"
      ],
      "metadata": {
        "id": "HESsZ_jEdAAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_and_chunk_text_from_pdf(pdf_path, chunk_size=200):\n",
        "    document = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(document)):\n",
        "        page = document.load_page(page_num)\n",
        "        text += page.get_text()\n",
        "\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    chunks = [' '.join(sentences[i:i + chunk_size]) for i in range(0, len(sentences), chunk_size)]\n",
        "    print(f\"Extracted and chunked text from {pdf_path}. Number of chunks: {len(chunks)}\")\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "0gytSrq_WmQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5. Embedding and Retrieval**"
      ],
      "metadata": {
        "id": "BlbQCtm0dNNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModelForCausalLM.from_pretrained('sentence-transformers/all-MiniLM-L6-v2').to('cuda')  # Move model to GPU\n",
        "\n",
        "def embed_text(texts):\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True).to('cuda')  # Move inputs to GPU\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()  # Move embeddings to CPU\n",
        "    return embeddings\n",
        "\n",
        "index = faiss.IndexFlatL2(384)  # Dimension should match the embedding size\n",
        "\n",
        "def load_chunks_and_index(db_name=\"documents.db\"):\n",
        "    conn = sqlite3.connect(db_name)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"SELECT content FROM chunks\")\n",
        "    chunks = [row[0] for row in cursor.fetchall()]\n",
        "    conn.close()\n",
        "\n",
        "    if chunks:\n",
        "        embeddings = embed_text(chunks)\n",
        "        index.add(embeddings)\n",
        "        print(f\"Loaded {len(chunks)} chunks and added to FAISS index.\")\n",
        "    else:\n",
        "        print(\"No chunks loaded from the database.\")\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "Snw11wFGWmO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6. Retrieval and Ranking**"
      ],
      "metadata": {
        "id": "SIVI7UacdbkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def retrieve_and_rank(chunks, query, top_k=5):\n",
        "    query_embedding = embed_text([query])\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    if len(indices[0]) == 0:\n",
        "        print(\"No chunks retrieved from the index.\")\n",
        "        return []\n",
        "\n",
        "    retrieved_chunks = [chunks[i] for i in indices[0] if i < len(chunks)]\n",
        "\n",
        "    if not retrieved_chunks:\n",
        "        print(\"No valid chunks retrieved after filtering.\")\n",
        "        return []\n",
        "\n",
        "    chunk_embeddings = embed_text(retrieved_chunks)\n",
        "    similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]\n",
        "    ranked_chunks = [retrieved_chunks[i] for i in np.argsort(similarities)[::-1]]\n",
        "\n",
        "    return ranked_chunks"
      ],
      "metadata": {
        "id": "Xo5r5usDWmND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**7. Generate Responses with LLaMA**"
      ],
      "metadata": {
        "id": "be7kYpSddpSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "access_token = \"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\"\n",
        "model_name = 'meta-llama/Meta-Llama-3-8B'  # Replace with the actual model name you are using\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=access_token)\n",
        "model = LlamaForCausalLM.from_pretrained(model_name, use_auth_token=access_token).to('cuda')\n",
        "\n",
        "def generate_response(chunks, query, top_k=5, prompt=\"Answer the following question based on the provided context:\"):\n",
        "    ranked_chunks = retrieve_and_rank(chunks, query, top_k)\n",
        "\n",
        "    if not ranked_chunks:\n",
        "        return \"No relevant chunks found to generate a response.\"\n",
        "\n",
        "    context = \" \".join(ranked_chunks) + \"\\n\" + prompt + \"\\n\" + query\n",
        "\n",
        "    inputs = tokenizer(context, return_tensors='pt').to('cuda')\n",
        "    outputs = model.generate(inputs.input_ids, max_new_tokens=150, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.strip()"
      ],
      "metadata": {
        "id": "vdeWEB0sWl1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**8. RAG Pipeline Function**"
      ],
      "metadata": {
        "id": "sM2Gm6nKd1rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_pipeline(pdf_paths, query, top_k=5, chunk_size=200, prompt=\"Answer the following question based on the provided context:\"):\n",
        "    create_database()\n",
        "    for pdf_path in pdf_paths:\n",
        "        chunks = extract_and_chunk_text_from_pdf(pdf_path, chunk_size)\n",
        "        insert_chunks(chunks)\n",
        "\n",
        "    chunks = load_chunks_and_index()\n",
        "\n",
        "    response = generate_response(chunks, query, top_k, prompt)\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "kh3xKLafWlic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**9. Upload Multiple PDFs**"
      ],
      "metadata": {
        "id": "x7uwxryld-N5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "pdf_paths = list(uploaded.keys())"
      ],
      "metadata": {
        "id": "-DSb2Zv1Wled"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**10.  Run the RAG Pipeline**"
      ],
      "metadata": {
        "id": "TRqPn2R0eGMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [\n",
        "    \"Configuration File Syntax in linux?\",\n",
        "    \"network configuration utility (ncat)\",\n",
        "    \"Basic requirements and setup for linux?\",\n",
        "    \"Why Guest Security Matters in linux\"\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    response = rag_pipeline(pdf_paths, query)\n",
        "    print('\\n', '\\n')\n",
        "    print('*' * 100)\n",
        "    print('Query: ', query)\n",
        "    print('*'*100)\n",
        "    print('\\n')\n",
        "    print('-'*100)\n",
        "    print('Response: ', response)\n",
        "    print('*' * 100)"
      ],
      "metadata": {
        "id": "WuCmXsDHWlUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**11.  Evaluate The Model**"
      ],
      "metadata": {
        "id": "ckV2mp5WeNuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_with_cosine_similarity(ground_truth, queries, pdf_paths, top_k=5, chunk_size=200, prompt=\"Answer the following question based on the provided context:\"):\n",
        "    create_database()\n",
        "    for pdf_path in pdf_paths:\n",
        "        chunks = extract_and_chunk_text_from_pdf(pdf_path, chunk_size)\n",
        "        insert_chunks(chunks)\n",
        "\n",
        "    chunks = load_chunks_and_index()\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for query in queries:\n",
        "        ranked_chunks = retrieve_and_rank(chunks, query, top_k)\n",
        "\n",
        "        if not ranked_chunks:\n",
        "            results[query] = {\"response\": \"No relevant chunks found to generate a response.\", \"similarity\": 0.0}\n",
        "            continue\n",
        "\n",
        "        context = \" \".join(ranked_chunks) + \"\\n\" + prompt + \"\\n\" + query\n",
        "        inputs = tokenizer(context, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "\n",
        "        outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Calculate cosine similarity with ground truth\n",
        "        ground_truth_embedding = embed_text([ground_truth[query]])\n",
        "        response_embedding = embed_text([response])\n",
        "        similarity = cosine_similarity(ground_truth_embedding, response_embedding)[0][0]\n",
        "\n",
        "        results[query] = {\"response\": response, \"similarity\": similarity}\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example Usage:\n",
        "pdf_paths = [\"Red_Hat_Enterprise_Linux-7-Migration_Planning_Guide-en-US.pdf\", \"Red_Hat_Enterprise_Linux-7-Virtualization_Getting_Started_Guide-en-US.pdf\", \"Red_Hat_Enterprise_Linux-7-Virtualization_Security_Guide-en-US.pdf\"]\n",
        "queries = [\"What is the syntax for configuration files in Linux?\", \"How to configure network utility (ncat) in Linux?\", \"Basic requirements and setup for linux?\", \"Why Guest Security Matters in linux\"]\n",
        "ground_truth = {\n",
        "    \"What is the syntax for configuration files in Linux?\": \"The syntax for configuration files in Linux typically involves directives, parameters, and comments.\",\n",
        "    \"How to configure network utility (ncat) in Linux?\": \"To configure ncat, you need to specify the target host, port, and any desired options such as verbosity or protocols.\",\n",
        "    \"Basic requirements and setup for linux?\": \"Setting up a Linux system involves several steps and requirements.\",\n",
        "    \"Why Guest Security Matters in linux\": \"Guest security is an important aspect of overall system security, especially in Linux environments that may be used by multiple users or that provide guest access. Here are several reasons why guest security matters in Linux.\"\n",
        "}\n",
        "\n",
        "evaluation_results = evaluate_with_cosine_similarity(ground_truth, queries, pdf_paths)\n",
        "for query, result in evaluation_results.items():\n",
        "    #print(f\"Query: {query}\\nResponse: {result['response']}\\nSimilarity: {result['similarity']}\\n\")\n",
        "\n",
        "    print('*'*100)\n",
        "    print('Query: ', {query}, '\\n')\n",
        "    print('*'*100)\n",
        "\n",
        "    print('*'*100)\n",
        "    print('Response: ', {result['response']}, '\\n')\n",
        "    print('*'*100)\n",
        "\n",
        "    print('*'*100)\n",
        "    print('Similarity: ', {result['similarity']}, '\\n')\n",
        "    print('*'*100)"
      ],
      "metadata": {
        "id": "qi1EeX6RWkuG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}